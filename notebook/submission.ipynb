{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# from tools import *\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GroupShuffleSplit, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "# From: https://www.kaggle.com/siddhrath/lightgbm-full-pipeline-model\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "\tnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\tstart_mem = df.memory_usage().sum() / 1024**2    \n",
    "\tfor col in df.columns:\n",
    "\t\tcol_type = df[col].dtypes\n",
    "\t\tif col_type in numerics:\n",
    "\t\t\tc_min = df[col].min()\n",
    "\t\t\tc_max = df[col].max()\n",
    "\t\t\tif str(col_type)[:3] == 'int':\n",
    "\t\t\t\tif c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "\t\t\t\t\tdf[col] = df[col].astype(np.int8)\n",
    "\t\t\t\telif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "\t\t\t\t\tdf[col] = df[col].astype(np.int16)\n",
    "\t\t\t\telif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "\t\t\t\t\tdf[col] = df[col].astype(np.int32)\n",
    "\t\t\t\telif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "\t\t\t\t\tdf[col] = df[col].astype(np.int64)  \n",
    "\t\t\telse:\n",
    "\t\t\t\tif c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "\t\t\t\t\tdf[col] = df[col].astype(np.float16)\n",
    "\t\t\t\telif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "\t\t\t\t\tdf[col] = df[col].astype(np.float32)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdf[col] = df[col].astype(np.float64)    \n",
    "\tend_mem = df.memory_usage().sum() / 1024**2\n",
    "\tif verbose: \n",
    "\t\tprint('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "\treturn df\n",
    "\n",
    "\n",
    "def get_portion(df, r=0.01):\n",
    "\tmols = list(set(df.molecule_name))\n",
    "\tshuffle(mols)\n",
    "\treturn df[df.molecule_name.isin(mols[:int(len(mols) * r)])].copy()\n",
    "\n",
    "\n",
    "\t# From: https://www.kaggle.com/seriousran/just-speed-up-calculate-distance-from-benchmark\n",
    "def map_atom_info(df, df_structures, atom_idx):\n",
    "\tdf = pd.merge(df, df_structures, how = 'left',\n",
    "\t\t\t\t  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "\t\t\t\t  right_on = ['molecule_name',  'atom_index'])\n",
    "\t\n",
    "\tdf = df.drop('atom_index', axis=1)\n",
    "\tdf = df.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "\t\t\t\t\t\t\t'x': f'x_{atom_idx}',\n",
    "\t\t\t\t\t\t\t'y': f'y_{atom_idx}',\n",
    "\t\t\t\t\t\t\t'z': f'z_{atom_idx}'})\n",
    "\treturn df\n",
    "\n",
    "\n",
    "# From: https://www.kaggle.com/seriousran/just-speed-up-calculate-distance-from-benchmark\n",
    "def get_dists(df, df_structures):\n",
    "\tdf = map_atom_info(df, df_structures, 0)\n",
    "\tdf = map_atom_info(df, df_structures, 1)\n",
    "\ttrain_p_0 = df[['x_0', 'y_0', 'z_0']].values\n",
    "\ttrain_p_1 = df[['x_1', 'y_1', 'z_1']].values\n",
    "\tdf['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\n",
    "\treturn df\n",
    "\n",
    "\n",
    "def atoms_per_molecule(df):\n",
    "\tdef core_atoms_per_molecule(df, atom):\n",
    "\t\tqry = df.query(f'atom_1 == \"{atom}\"')\n",
    "\t\tcounts = qry.groupby('molecule_name')['atom_index_1'].nunique().to_dict()\n",
    "\t\tdf['n_' + atom] = df['molecule_name'].apply(lambda x: counts.get(x, 0))\n",
    "\t\treturn df\n",
    "\tfor atom in df.atom_1.unique():\n",
    "\t\tdf = core_atoms_per_molecule(df, atom)\n",
    "\treturn df\n",
    "\n",
    "\n",
    "def plist(foolist):\n",
    "    for a,b,c in zip(foolist[::3],foolist[1::3],foolist[2::3]):\n",
    "        print('{:<20}{:<20}{:<}'.format(a,b,c))\n",
    "\n",
    "\n",
    "def importances(feats, model):\n",
    "    return pd.DataFrame(index=feats, data=model.feature_importances_,\n",
    "                        columns=['importance']).sort_values('importance')[::-1]\n",
    "\n",
    "\n",
    "def encode_cols(df, cols):\n",
    "    for c in cols:\n",
    "        encoded = pd.get_dummies(df[c], prefix=c, sparse=True)\n",
    "        df = df.join(encoded)\n",
    "    return df.drop(cols, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv('../input/train.csv', index_col='id')\n",
    "df_structures = pd.read_csv('../input/structures.csv');\n",
    "df_scc = pd.read_csv('../input/scalar_coupling_contributions.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_keys = ['molecule_name', 'atom_index_0', 'atom_index_1', 'type']\n",
    "df_orig = pd.merge(df_orig, df_scc, how = 'left', left_on= join_keys, right_on= join_keys)\n",
    "del df_scc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_orig = reduce_mem_usage(df_orig)\n",
    "df_structures = reduce_mem_usage(df_structures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_orig #get_portion(df_orig, r=0.05)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = get_dists(df, df_structures)\n",
    "df = atoms_per_molecule(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Stats\n",
    "def get_descriptive_stats(df):\n",
    "#     df['mean_dist_by_atom'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('mean')\n",
    "#     df['max_dist_by_atom'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('max')\n",
    "#     df['min_dist_by_atom'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('mean')\n",
    "#     df['std_dist_by_atom'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('std').fillna(0)\n",
    "    \n",
    "    df['mean_dist_by_mol'] = df.groupby(['molecule_name'])['dist'].transform('mean')\n",
    "    df['max_dist_by_mol'] = df.groupby(['molecule_name'])['dist'].transform('max')\n",
    "    df['min_dist_by_mol'] = df.groupby(['molecule_name'])['dist'].transform('mean')\n",
    "    df['std_dist_by_mol'] = df.groupby(['molecule_name'])['dist'].transform('std').fillna(0)\n",
    "    return df\n",
    "df = get_descriptive_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_neighbours(df):\n",
    "    for atom in df.atom_1.unique():\n",
    "        df_nearby = pd.DataFrame(df.query(f'atom_1 == \"{atom}\"').groupby(['molecule_name', 'atom_index_1', 'atom_0'])['atom_0'].size())\n",
    "        df_nearby = df_nearby.fillna(0)\n",
    "        df_nearby.columns = [f'nearby_{atom}']\n",
    "        df = pd.merge(df, df_nearby, how='left', on=['molecule_name', 'atom_index_1', 'atom_0']).fillna(0)\n",
    "    return df\n",
    "df = get_num_neighbours(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats = [\n",
    "#     'molecule_name',\n",
    "#     'atom_index_0',\n",
    "#     'atom_index_1',\n",
    "    'type',\n",
    "#     'scalar_coupling_constant',\n",
    "#     'fc',\n",
    "#     'sd',\n",
    "#     'pso',\n",
    "#     'dso',\n",
    "#     'atom_0',\n",
    "    'x_0',\n",
    "    'y_0',\n",
    "    'z_0',\n",
    "    'atom_1',\n",
    "    'x_1',\n",
    "    'y_1',\n",
    "    'z_1',\n",
    "    'dist',\n",
    "    'n_C',\n",
    "    'n_H',\n",
    "    'n_N',\n",
    "#     'mean_dist_by_atom',\n",
    "#     'max_dist_by_atom',\n",
    "#     'min_dist_by_atom',\n",
    "#     'std_dist_by_atom',\n",
    "    'mean_dist_by_mol',\n",
    "    'max_dist_by_mol',\n",
    "    'min_dist_by_mol',\n",
    "    'std_dist_by_mol',\n",
    "    'nearby_C',\n",
    "    'nearby_N',\n",
    "    'nearby_H'\n",
    "]\n",
    "categoricals = ['type', 'atom_1']\n",
    "target = ['fc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Categoricals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categoricals = ['type', 'atom_1']\n",
    "feats = list(set(all_feats) - set(categoricals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_model(X, feats, constant):\n",
    "    assert constant in df.columns\n",
    "    print(f'Returning model to generate {constant}.')\n",
    "    rf = RandomForestRegressor(n_jobs=-1)\n",
    "    gss = GroupShuffleSplit(n_splits=3)\n",
    "    scores = cross_val_score(rf, X[feats], y, scoring='neg_mean_absolute_error',\n",
    "                            groups=X['molecule_name'], cv=gss, n_jobs=-1)\n",
    "    rf.fit(X[feats], y)\n",
    "    print(np.mean(np.log(-scores)))\n",
    "    return rf\n",
    "from joblib import dump, load\n",
    "def save_model(model, fname):\n",
    "    dump(model, f'{fname}.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Main Data and Test Data and Compute Constants now to Delete Model Right After (and save RAM)\n",
    "def featurize(df_, categoricals=None):\n",
    "    # Feature Engineering\n",
    "    df_ = get_dists(df_, df_structures)\n",
    "    df_ = atoms_per_molecule(df_)\n",
    "    df_ = get_descriptive_stats(df_)\n",
    "    df_ = get_num_neighbours(df_)\n",
    "    # Encode Categoricals\n",
    "    df_ = encode_cols(df_, categoricals)\n",
    "    assert not df.isna().all().any(), \"NaNs present in DataFrame\"\n",
    "    return df_\n",
    "df = featurize(df_orig, categoricals)\n",
    "X_final = featurize(pd.read_csv('../input/test.csv'), categoricals)\n",
    "df = reduce_mem_usage(df)\n",
    "X_final = reduce_mem_usage(X_final)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split for training meta-features\n",
    "X = df.loc[:, feats + ['molecule_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = 'fc'\n",
    "print('TARGET:', target[0])\n",
    "print(f'FEATURES (n = {len(X.columns)}):')\n",
    "plist(X.columns)\n",
    "print(75*'-')\n",
    "y = df.loc[:, [target]]\n",
    "fc_model = constant_model(X, feats, target)\n",
    "save_model(fc_model, 'fc_model')\n",
    "# Generate Meta-Feature on Train Data\n",
    "df[f\"{target}_pred\"] = fc_model.predict(X[feats])\n",
    "# Generate Meta-Feature on Submission Data\n",
    "X_final[f\"{target}_pred\"] = fc_model.predict(X_final[feats])\n",
    "del fc_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = 'sd'\n",
    "print('TARGET:', target[0])\n",
    "print(f'FEATURES (n = {len(X.columns)}):')\n",
    "plist(X.columns)\n",
    "print(75*'-')\n",
    "y = df.loc[:, [target]]\n",
    "sd_model = constant_model(X, feats, target)\n",
    "save_model(sd_model, 'sd_model')\n",
    "# Generate Meta-Feature on Train Data\n",
    "df[f\"{target}_pred\"] = sd_model.predict(X[feats])\n",
    "# Generate Meta-Feature on Submission Data\n",
    "X_final[f\"{target}_pred\"] = sd_model.predict(X_final[feats])\n",
    "del sd_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = 'pso'\n",
    "print('TARGET:', target[0])\n",
    "print(f'FEATURES (n = {len(X.columns)}):')\n",
    "plist(X.columns)\n",
    "print(75*'-')\n",
    "y = df.loc[:, [target]]\n",
    "pso_model = constant_model(X, feats, target)\n",
    "save_model(pso_model, 'pso_model')\n",
    "# Generate Meta-Feature on Train Data\n",
    "df[f\"{target}_pred\"] = pso_model.predict(X[feats])\n",
    "# Generate Meta-Feature on Submission Data\n",
    "X_final[f\"{target}_pred\"] = pso_model.predict(X_final[feats])\n",
    "del pso_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = 'dso'\n",
    "print('TARGET:', target[0])\n",
    "print(f'FEATURES (n = {len(X.columns)}):')\n",
    "plist(X.columns)\n",
    "print(75*'-')\n",
    "y = df.loc[:, [target]]\n",
    "dso_model = constant_model(X, feats, target)\n",
    "save_model(dso_model, 'dso_model')\n",
    "# Generate Meta-Feature on Train Data\n",
    "df[f\"{target}_pred\"] = dso_model.predict(X[feats])\n",
    "# Generate Meta-Feature on Submission Data\n",
    "X_final[f\"{target}_pred\"] = dso_model.predict(X_final[feats])\n",
    "del dso_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_feats = ['fc_pred', 'sd_pred', 'pso_pred', 'dso_pred']\n",
    "meta_orig = ['fc', 'sd', 'pso', 'dso']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model (Scalar Coupling Constant) using meta-feats to see if overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split Target\n",
    "final_targ = ['scalar_coupling_constant']\n",
    "# Reduce Memory\n",
    "df = reduce_mem_usage(df)\n",
    "X_ = df.loc[:, feats + meta_feats + ['molecule_name']]\n",
    "y_ = df.loc[:, final_targ]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_jobs=-1)\n",
    "gss = GroupShuffleSplit(n_splits=3)\n",
    "train_idxs, test_idxs = next(gss.split(X_['molecule_name'], y_, groups=X_['molecule_name']))\n",
    "X_train, X_test, y_train, y_test = X_.iloc[train_idxs], X_.iloc[test_idxs], y_.iloc[train_idxs], y_.iloc[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train[feats + meta_feats], y_train)\n",
    "y_pred = model.predict(X_test[feats + meta_feats])\n",
    "np.log(mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain on all data to save for later analysis\n",
    "model.fit(X_[feats + meta_feats], y_)\n",
    "y_pred = model.predict(X_[feats + meta_feats])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now Train Final Model, and use original constants instead of meta_features\n",
    "X_ = df.loc[:, feats + meta_orig]\n",
    "y_ = df.loc[:, final_targ]\n",
    "model.fit(X_[feats + meta_orig], y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(df.loc[:, feats + meta_feats])\n",
    "np.log(mean_absolute_error(y_, y_pred)) # See performance on meta features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_, y_, X_train, X_test, y_train, y_test, df_orig\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final_ids = X_final.loc[:, 'id']\n",
    "X_final = X_final.drop(['id'], axis=1)\n",
    "\n",
    "# Prepare for Predictions\n",
    "X_final = X_final.loc[:, feats + meta_feats]\n",
    "X_final = reduce_mem_usage(X_final)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final = pd.DataFrame(np.nan_to_num(X_final[feats + meta_feats]), columns=feats + meta_feats)\n",
    "print(X_final.columns)\n",
    "pred = model.predict(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission = pd.concat([X_final_ids, pd.Series(pred)], axis=1)\n",
    "final_submission.columns = ['id', 'scalar_coupling_constant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission.to_csv('submission.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
